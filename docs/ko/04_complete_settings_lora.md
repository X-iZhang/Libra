# Libra ì™„ì „ ì„¤ì • ê°€ì´ë“œ & LoRA êµ¬í˜„

## ëª©ì°¨
- [Vision Encoderë³„ Input Size](#vision-encoderë³„-input-size)
- [ì‚¬ìš© ê°€ëŠ¥í•œ Vision Encoders](#ì‚¬ìš©-ê°€ëŠ¥í•œ-vision-encoders)
- [ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸](#ì‚¬ìš©-ê°€ëŠ¥í•œ-llm-ëª¨ë¸)
- [í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •](#í•˜ì´í¼íŒŒë¼ë¯¸í„°-ì„¤ì •)
- [LoRA êµ¬í˜„ ìƒì„¸](#lora-êµ¬í˜„-ìƒì„¸)

---

## Vision Encoderë³„ Input Size

### ì™„ì „íˆ ë‹¤ë¦…ë‹ˆë‹¤!

| Vision Encoder | Input Size | Patch Size | Num Patches | Hidden Dim | Num Layers |
|---------------|-----------|------------|-------------|------------|------------|
| **RAD-DINO** | **518Ã—518** | 14Ã—14 | 1369 (37Ã—37) | **768** | **12** |
| **CLIP-ViT-Large-336** | **336Ã—336** | 14Ã—14 | 576 (24Ã—24) | **1024** | 24 |
| **CLIP-ViT-Large-224** | 224Ã—224 | 14Ã—14 | 256 (16Ã—16) | 1024 | 24 |
| **BiomedCLIP** | 224Ã—224 | 16Ã—16 | 196 (14Ã—14) | 768 | 12 |
| **SigLIP** | 384Ã—384 | 16Ã—16 | 576 (24Ã—24) | 1152 | 27 |

**ì½”ë“œì—ì„œ ìë™ ì²˜ë¦¬** (dino_encoder.py:135):
```python
@property
def num_patches(self):
    return (self.config.image_size // self.config.patch_size) ** 2
```

**ì£¼ì˜ì‚¬í•­**:
- TACëŠ” RAD-DINO (12 layers) ê¸°ì¤€ìœ¼ë¡œ í•˜ë“œì½”ë”©ë¨ (builder.py:29)
- ë‹¤ë¥¸ encoder ì‚¬ìš© ì‹œ `layers_number` ìˆ˜ì • í•„ìš”

---

## ì‚¬ìš© ê°€ëŠ¥í•œ Vision Encoders

| Encoder | HuggingFace ID | Image Size | Hidden Size | íŠ¹ì§• |
|---------|---------------|------------|-------------|------|
| **RAD-DINO** â­ | `microsoft/rad-dino` | 518Ã—518 | 768 | Radiology ì „ë¬¸, SOTA |
| **CLIP-Large** | `openai/clip-vit-large-patch14-336` | 336Ã—336 | 1024 | ë²”ìš©, ëŒ€ê·œëª¨ |
| **CLIP-Base** | `openai/clip-vit-base-patch16` | 224Ã—224 | 768 | ê²½ëŸ‰ |
| **BiomedCLIP** | `microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224` | 224Ã—224 | 768 | ì˜ë£Œ ì „ë¬¸ |
| **SigLIP** | `google/siglip-so400m-patch14-384` | 384Ã—384 | 1152 | ìµœì‹ , íš¨ìœ¨ì  |

**ì„¤ì • ë°©ë²•**:
```bash
# pretrain.sh or finetune_lora.sh
VISION_TOWER="microsoft/rad-dino"  # ë³€ê²½ ê°€ëŠ¥
```

---

## ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸

| LLM Family | íŒŒì¼ | HuggingFace Example | Hidden Size |
|-----------|------|-------------------|-------------|
| **LLaMA** | `libra_llama.py` | `meta-llama/Llama-2-7b-chat-hf`<br>`meta-llama/Llama-3-8B-Instruct`<br>`meta-llama/Llama-3.2-3B-Instruct` | 4096 (7B)<br>3072 (3B) |
| **Meditron** â­ | `libra_llama.py` | `epfl-llm/meditron-7b` | 4096 |
| **Vicuna** | `libra_llama.py` | `lmsys/vicuna-7b-v1.5` | 4096 |
| **Mistral** | `libra_mistral.py` | `mistralai/Mistral-7B-Instruct-v0.2` | 4096 |
| **Qwen2** | `libra_qwen2.py` | `Qwen/Qwen2.5-3B-Instruct` | 2048 |
| **Qwen3** | `libra_qwen3.py` | `Qwen/Qwen3-4B-Instruct-2507` | 3072 |
| **Phi-3** | `libra_phi3.py` | `microsoft/Phi-3-mini-4k-instruct` | 3072 |
| **Gemma** | `libra_gemma.py` | `google/gemma-2-2b-it` | 2304 |

**ì„¤ì • ë°©ë²•** (pretrain.sh:18-42):
```bash
# Meditron (ê¶Œì¥)
MODEL_VERSION="epfl-llm/meditron-7b"
PROMPT_VERSION="libra_v1"

# LLaMA-3
MODEL_VERSION="meta-llama/Llama-3-8B-Instruct"
PROMPT_VERSION="llama_3"

# Qwen3
MODEL_VERSION="Qwen/Qwen3-4B-Instruct-2507"
PROMPT_VERSION="qwen"
```

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

### Stage 1 (Pretrain) - `scripts/pretrain.sh`

```bash
###############################################################################
# Stage 1: Visual Feature Alignment (TAC í•™ìŠµ)
###############################################################################

# â•â•â• ë°ì´í„° â•â•â•
TRAIN_DATA="./path/to/libra_alignment_train.json"
VAL_DATA="./path/to/libra_alignment_valid.json"
IMG_FOLDER="./path/to/mimic-cxr-jpg/2.0.0"

# â•â•â• ëª¨ë¸ êµ¬ì„± â•â•â•
MODEL_VERSION="epfl-llm/meditron-7b"
VISION_TOWER="microsoft/rad-dino"
PROMPT_VERSION="libra_v1"

# â•â•â• í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° â•â•â•
NUM_EPOCHS=1                    # Epoch
TRAIN_BSZ=16                    # Per-device batch size
EVAL_BSZ=4                      # Eval batch size
GRAD_ACC_STEPS=1                # Gradient accumulation
LR=2e-5                         # Learning rate (TAC)
WEIGHT_DECAY=0.                 # Weight decay
WARMUP_RATIO=0.03               # Warmup (3%)
LR_SCHEDULER="cosine"           # Scheduler type
MAX_LENGTH=2048                 # Token length

# â•â•â• ìµœì í™” ì„¤ì • â•â•â•
DEEPSPEED_CONFIG="./scripts/zero2.json"
BF16=True                       # bfloat16 precision
TF32=True                       # TF32 for A100
GRADIENT_CHECKPOINTING=True     # Memory ì ˆì•½

# â•â•â• TAC í•™ìŠµ í”Œë˜ê·¸ â•â•â•
--freeze_backbone True          # â„ï¸ LLM frozen
--tune_mm_mlp_adapter True      # ğŸ”¥ TAC trainable
--freeze_mm_mlp_adapter False   # ğŸ”¥ TAC trainable
--mm_projector_type TAC         # Projector type
--mm_vision_select_layer all    # All 12 layers

# â•â•â• ì €ì¥/í‰ê°€ â•â•â•
--save_steps 20000              # Save every 20K steps
--save_total_limit 1            # Keep only 1 checkpoint
--eval_strategy "steps"         # Eval during training
--eval_steps 0.01               # Eval every 1%
--compute_metrics True          # Calculate BLEU, etc.
```

---

### Stage 2 (LoRA Finetune) - `scripts/finetune_lora.sh`

```bash
###############################################################################
# Stage 2: Downstream Task Fine-tuning (LLM í•™ìŠµ)
###############################################################################

# â•â•â• ë°ì´í„° â•â•â•
TRAIN_DATA="./path/to/libra_findings_section_train.json"
VAL_DATA="./path/to/libra_findings_section_valid.json"
IMG_FOLDER="./path/to/mimic-cxr-jpg/2.0.0"

# â•â•â• ëª¨ë¸ êµ¬ì„± â•â•â•
MODEL_VERSION="./checkpoints/libra-v1.0-7b-pretrain"
VISION_TOWER="microsoft/rad-dino"
PROMPT_VERSION="libra_v1"

# â•â•â• LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„° â­ â•â•â•
LORA_R=128                      # LoRA rank
LORA_ALPHA=256                  # LoRA alpha (scaling=2.0)
LORA_DROPOUT=0.05               # LoRA dropout
MM_PROJECTOR_LR=2e-5            # TAC learning rate (optional)

# â•â•â• í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° â•â•â•
NUM_EPOCHS=3                    # Epoch (ë” ë§ìŒ)
TRAIN_BSZ=16                    # Per-device batch size
EVAL_BSZ=4
GRAD_ACC_STEPS=1
LR=2e-5                         # Learning rate (LoRA)
WEIGHT_DECAY=0.
WARMUP_RATIO=0.03
LR_SCHEDULER="cosine"
MAX_LENGTH=2048

# â•â•â• ìµœì í™” ì„¤ì • â•â•â•
DEEPSPEED_CONFIG="./scripts/zero3.json"  # Zero-3
BF16=True
TF32=True
GRADIENT_CHECKPOINTING=True

# â•â•â• LoRA í•™ìŠµ í”Œë˜ê·¸ â•â•â•
--lora_enable True              # ğŸ”¥ LoRA í™œì„±í™”
--lora_r ${LORA_R}
--lora_alpha ${LORA_ALPHA}
--mm_projector_lr ${MM_PROJECTOR_LR}
--freeze_backbone True          # â„ï¸ LLM backbone frozen
--tune_mm_mlp_adapter False     # â„ï¸ TAC frozen
--freeze_mm_mlp_adapter True    # â„ï¸ TAC frozen

# â•â•â• ì €ì¥/í‰ê°€ â•â•â•
--save_steps 2000
--save_total_limit 1
--eval_strategy "steps"
--eval_steps 0.01
--compute_metrics True
```

---

## LoRA êµ¬í˜„ ìƒì„¸

### LoRA ì´ˆê¸°í™” ê³¼ì •

**ìœ„ì¹˜**: `train.py:1626-1642`

```python
if training_args.lora_enable:
    from peft import LoraConfig, get_peft_model

    # Step 1: LoRA Config ìƒì„±
    lora_config = LoraConfig(
        r=training_args.lora_r,              # 128 (rank)
        lora_alpha=training_args.lora_alpha, # 256 (scaling factor)
        target_modules=find_all_linear_names(model),
        lora_dropout=training_args.lora_dropout,      # 0.05
        bias=training_args.lora_bias,                 # "none"
        task_type="CAUSAL_LM",
    )

    # Step 2: PEFT ëª¨ë¸ë¡œ ë³€í™˜
    model = get_peft_model(model, lora_config)
```

---

### Target Modules ìë™ íƒì§€

**ìœ„ì¹˜**: `train.py:222-235`

```python
def find_all_linear_names(model):
    cls = torch.nn.Linear
    lora_module_names = set()

    # âŒ ì œì™¸í•  ëª¨ë“ˆ
    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']

    for name, module in model.named_modules():
        # Vision/Projector ëª¨ë“ˆì€ ê±´ë„ˆë›°ê¸°
        if any(mm_keyword in name for mm_keyword in multimodal_keywords):
            continue

        # âœ… Linear ë ˆì´ì–´ë§Œ ì„ íƒ
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])

    # lm_headë„ ì œì™¸
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')

    return list(lora_module_names)
```

**ê²°ê³¼** (Meditron-7B):
```python
target_modules = [
    'q_proj',      # Query projection
    'k_proj',      # Key projection
    'v_proj',      # Value projection
    'o_proj',      # Output projection
    'gate_proj',   # Gate projection (MLP)
    'up_proj',     # Up projection (MLP)
    'down_proj'    # Down projection (MLP)
]
```

---

### LoRA ì ìš© êµ¬ì¡°

#### ì›ë˜ Attention Layer
```
Input (4096)
    â†“
[Q Projection] (4096 â†’ 4096)  â† 32M params (frozen)
```

#### LoRA ì ìš© í›„
```
Input (4096)
    â†“
[Q Projection (frozen)] â”€â”¬â”€â†’ Output
                          â”‚
                          â””â”€â†’ [LoRA_A: 4096â†’128] â†’ [LoRA_B: 128â†’4096]
                               (524K params, trainable)
```

**ìˆ˜ì‹**:
```
h = Wâ‚€x + (Î±/r) Â· Î”Wx
  = Wâ‚€x + (256/128) Â· BÂ·AÂ·x

where:
  Wâ‚€: frozen weights (4096Ã—4096)
  A: trainable (4096Ã—128)
  B: trainable (128Ã—4096)
  Î±: scaling factor (256)
  r: rank (128)
```

---

### í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„° ê³„ì‚°

**Meditron-7B ê¸°ì¤€**:

```python
# 32ê°œ Transformer layers
# ê° layerë‹¹:
#   - q_proj, k_proj, v_proj, o_proj (Attention): 4ê°œ
#   - gate_proj, up_proj, down_proj (MLP): 3ê°œ
# ì´: 7ê°œ modules per layer

# ê° moduleë‹¹ LoRA íŒŒë¼ë¯¸í„°:
#   A: 4096 Ã— 128 = 524,288
#   B: 128 Ã— 4096 = 524,288
#   Total per module: 1,048,576 (~1M)

# ì „ì²´ í•™ìŠµ íŒŒë¼ë¯¸í„°:
32 layers Ã— 7 modules Ã— 1M = 224M params
```

**ë©”ëª¨ë¦¬ íš¨ìœ¨**:
```
Full finetuning: 7B Ã— 4 bytes = 28GB
LoRA: 224M Ã— 4 bytes = 896MB

ì ˆì•½: 96.8%!
```

---

### LoRA Scaling Factor

```python
# finetune_lora.sh
LORA_R=128
LORA_ALPHA=256

# Effective scaling
scaling = Î± / r = 256 / 128 = 2.0
```

**ì˜ë¯¸**:
- LoRA ì¶œë ¥ì´ frozen weight ì¶œë ¥ì˜ **2ë°°** ìŠ¤ì¼€ì¼
- Î±ê°€ í´ìˆ˜ë¡ LoRA ê¸°ì—¬ë„ ì¦ê°€

---

### ì €ì¥ ë©”ì»¤ë‹ˆì¦˜

**ìœ„ì¹˜**: `train.py:1816-1826`

```python
if training_args.lora_enable:
    # LoRA íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥
    state_dict = get_peft_state_maybe_zero_3(
        model.named_parameters(),
        training_args.lora_bias
    )
    # íŒŒì¼ëª…: adapter_model.bin (~900MB)

    # Non-LoRA trainables ì €ì¥
    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(
        model.named_parameters()
    )
    torch.save(non_lora_state_dict,
               os.path.join(output_dir, 'non_lora_trainables.bin'))
```

**ì €ì¥ êµ¬ì¡°**:
```
./checkpoints/libra-v1.0-7b-lora/
â”œâ”€â”€ adapter_model.bin          # LoRA weights (224M params)
â”œâ”€â”€ adapter_config.json        # LoRA config
â”œâ”€â”€ non_lora_trainables.bin    # mm_projector
â””â”€â”€ config.json                # Model config
```

---

### Inference ì‹œ ë¡œë”©

**ìœ„ì¹˜**: `builder.py:51-83`

```python
if 'lora' in model_name.lower() and model_base is not None:
    # Step 1: Base model ë¡œë“œ
    model = LibraLlamaForCausalLM.from_pretrained(
        model_base,  # "epfl-llm/meditron-7b"
        config=lora_cfg_pretrained
    )

    # Step 2: Non-LoRA trainables ë¡œë“œ
    non_lora_trainables = torch.load(
        os.path.join(model_path, 'non_lora_trainables.bin')
    )
    model.load_state_dict(non_lora_trainables, strict=False)

    # Step 3: LoRA weights ë¡œë“œ
    from peft import PeftModel
    model = PeftModel.from_pretrained(model, model_path)

    # Step 4: LoRA merge (inference ì†ë„ í–¥ìƒ)
    model = model.merge_and_unload()
```

---

## ê° ë¸”ë¡ë³„ í•™ìŠµ ì„¤ì •

| ë¸”ë¡ | Stage 1 (Pretrain) | Stage 2 (LoRA Finetune) |
|------|-------------------|------------------------|
| **Vision Encoder** | â„ï¸ Frozen | â„ï¸ Frozen |
| **TAC (mm_projector)** | ğŸ”¥ **Trainable**<br>LR: 2e-5 | â„ï¸ Frozen |
| **LLM Backbone** | â„ï¸ Frozen | â„ï¸ Frozen |
| **LoRA Adapters** | âŒ ì—†ìŒ | ğŸ”¥ **Trainable**<br>LR: 2e-5<br>Rank: 128<br>Alpha: 256 |

---

## Libra vs Standard LoRA ë¹„êµ

| í•­ëª© | Standard LoRA | Libra LoRA |
|------|--------------|-----------|
| **Target** | q_proj, v_projë§Œ | 7ê°œ modules |
| **Rank** | 8-64 | **128** |
| **Alpha** | rê³¼ ë™ì¼ | **256** (2Ã—r) |
| **ì ìš© ë ˆì´ì–´** | Attentionë§Œ | Attention + MLP |
| **íŒŒë¼ë¯¸í„° ìˆ˜** | ~50M | **224M** |
| **ì„±ëŠ¥** | ì ë‹¹ | **ë†’ìŒ** |

---

## í•µì‹¬ ì„¤ì • ìš”ì•½

| ì„¤ì • | Stage 1 | Stage 2 (LoRA) | ë¹„ê³  |
|------|---------|---------------|------|
| **Learning Rate** | 2e-5 | 2e-5 | ë™ì¼ |
| **Batch Size** | 16 | 16 | Global |
| **Epochs** | 1 | 3 | Stage 2ê°€ 3ë°° |
| **Max Length** | 2048 | 2048 | Token limit |
| **LoRA Rank** | - | 128 | High rank |
| **LoRA Alpha** | - | 256 | Scaling=2.0 |
| **DeepSpeed** | Zero-2 | Zero-3 | Stage 2 ë” íš¨ìœ¨ |
| **Precision** | BF16 | BF16 | A100/A6000 |

---

## ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: [Libra (ACL 2025)](https://arxiv.org/abs/2411.19378)
- **ì½”ë“œ**: [GitHub](https://github.com/X-iZhang/Libra)
- **LoRA ë…¼ë¬¸**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **PEFT**: [HuggingFace PEFT Library](https://github.com/huggingface/peft)
