# Libra ì™„ì „ ë¶„ì„ & ì‹¤ì „ ê°€ì´ë“œ (í•œêµ­ì–´)

> **Libra**: Leveraging Temporal Images for Biomedical Radiology Analysis (ACL 2025)
>
> ë³¸ ë¬¸ì„œëŠ” Libra í”„ë¡œì íŠ¸ì˜ ì‹¬ì¸µ ë¶„ì„, í•™ìŠµ ê°€ì´ë“œ, ê·¸ë¦¬ê³  ìµœì í™” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ“š ë¬¸ì„œ ëª©ì°¨

### 1. [ì•„í‚¤í…ì²˜ & ì½”ë“œ êµ¬ì¡° ë¶„ì„](./01_architecture_analysis.md)
**ë‚´ìš©**:
- ë…¼ë¬¸ í•µì‹¬ ê°œë… (ACL 2025)
- ì „ì²´ ì•„í‚¤í…ì²˜ íŒŒì´í”„ë¼ì¸
- TAC (Temporal Alignment Connector) ìƒì„¸ ë¶„ì„
  - LFE (Layerwise Feature Extractor)
  - TFM (Temporal Feature Matching)
- ì½”ë“œ êµ¬ì¡° (2,220 lines)
- ì°¨ì› ë³€í™˜ ì¶”ì 
- í•µì‹¬ ì„¤ê³„ ê²°ì •

**ì£¼ìš” ë‚´ìš©**:
- TACê°€ 12ê°œ vision layerë¥¼ 1ê°œë¡œ ì••ì¶•
- Cosine Similarityë¥¼ 8ìŠ¹í•˜ëŠ” ì´ìœ 
- Cross-Attentionì´ ë‹¨ë°©í–¥ì¸ ì´ìœ 
- ì½”ë“œì™€ ë…¼ë¬¸ì˜ 1:1 ë§¤ì¹­

---

### 2. [MIMIC-CXR Temporal Pairing ë°©ë²•ë¡ ](./02_temporal_pairing.md)
**ë‚´ìš©**:
- Libraì˜ Prior Image Retrieval ì•Œê³ ë¦¬ì¦˜
- Same-day Studies íŠ¹ìˆ˜ ì²˜ë¦¬
- ë‹¤ë¥¸ ì—°êµ¬ë“¤ê³¼ ë¹„êµ (MLRG, TiBiX, CoCa-CXR)
- MIMIC-CXR í†µê³„
- ì‹¤ì „ ë°ì´í„° ì²˜ë¦¬ ì½”ë“œ

**ì£¼ìš” ë‚´ìš©**:
- `StudyDate` + `StudyTime` ê¸°ë°˜ pairing
- ê°™ì€ ë‚  ì—¬ëŸ¬ ì´¬ì˜ ì‹œ ë™ì¼í•œ prior ì‚¬ìš©
- 67% í™˜ìê°€ 2íšŒ ì´ìƒ ì´¬ì˜
- 100% ì¬í˜„ ê°€ëŠ¥í•œ Python ì½”ë“œ ì œê³µ

---

### 3. [í•™ìŠµ ê°€ì´ë“œ (Stage 1 & Stage 2)](./03_training_guide.md)
**ë‚´ìš©**:
- 2-Stage í•™ìŠµ ì „ëµ ì™„ì „ ë¶„ì„
- Stage 1: Visual Feature Alignment (TAC í•™ìŠµ)
- Stage 2: Downstream Task Fine-tuning (LoRA)
- í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸
- ì™„ì „í•œ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ì£¼ìš” ë‚´ìš©**:
- Stage 1: 385ì‹œê°„ (16ì¼), TACë§Œ í•™ìŠµ
- Stage 2: 213ì‹œê°„ (9ì¼), LLM LoRA í•™ìŠµ
- ì™œ 2-Stageê°€ í•„ìš”í•œê°€?
- ê³µê°œ weightsë¡œ ê° stage ê±´ë„ˆë›°ê¸° ê°€ëŠ¥

---

### 4. [ì™„ì „ ì„¤ì • ê°€ì´ë“œ & LoRA êµ¬í˜„](./04_complete_settings_lora.md)
**ë‚´ìš©**:
- Vision Encoderë³„ Input Size ë¹„êµ
- ì‚¬ìš© ê°€ëŠ¥í•œ Vision Encoders (RAD-DINO, CLIP, BiomedCLIP ë“±)
- ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ (Meditron, LLaMA, Qwen, Mistral ë“±)
- ì „ì²´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
- LoRA êµ¬í˜„ ìƒì„¸
  - Target Modules ìë™ íƒì§€
  - í•™ìŠµ íŒŒë¼ë¯¸í„° ê³„ì‚° (224M)
  - Scaling Factor (Î±/r = 2.0)

**ì£¼ìš” ë‚´ìš©**:
- RAD-DINO: 518Ã—518, 768 dim, 12 layers
- CLIP-Large: 336Ã—336, 1024 dim, 24 layers
- LoRA rank=128, alpha=256 (aggressive ì„¤ì •)
- 7ê°œ modules per layer (Attention + MLP)

---

### 5. [í‰ê°€ ì§€í‘œ ì™„ì „ ê°€ì´ë“œ](./05_evaluation_metrics.md)
**ë‚´ìš©**:
- BLEU (1, 2, 3, 4): n-gram precision
- METEOR: ë™ì˜ì–´, ì–´ê·¼ ê³ ë ¤
- ROUGE-L: ìµœì¥ ê³µí†µ ë¶€ë¶„ ìˆ˜ì—´
- RaTEScore: Radiology ì „ìš© (EMNLP 2024) â­
- RG_ER: RadGraph Entity Recall
- ì§€í‘œ ê°„ ê´€ê³„ & ì¢…í•© ë¶„ì„

**ì£¼ìš” ë‚´ìš©**:
- RaTEScoreê°€ ê°€ì¥ ì¤‘ìš” (ì„ìƒ í‰ê°€ì™€ highest correlation)
- BLEUëŠ” ì–¸ì–´ì  ìœ ì‚¬ì„±ë§Œ ì¸¡ì •
- RG_ERë¡œ ì™„ì „ì„± í‰ê°€
- Libra-v1.0-3b ì ìˆ˜ ìƒì„¸ í•´ì„

---

### 6. [H100 GPU ìµœì í™” ê°€ì´ë“œ](./06_h100_optimization.md)
**ë‚´ìš©**:
- GPU ìŠ¤í™ ë¹„êµ (A6000 vs H100)
- í•™ìŠµ ì‹œê°„ ê³„ì‚°
- ìµœì  GPU ìˆ˜ ì¶”ì²œ
- Multi-GPU ì„¤ì • ê°€ì´ë“œ
- ë¹„ìš© ë¹„êµ

**ì£¼ìš” ë‚´ìš©**:
- **H100 1ê°œ ì¶”ì²œ**: 4.2ì¼, $300, ê°€ì¥ íš¨ìœ¨ì 
- H100 vs A6000: ~6-7Ã— ë¹ ë¦„
- H100 2ê°œ: 2ì¼ ì™„ë£Œ (ê¸‰í•  ë•Œ)
- H100 4ê°œ ì´ìƒ: ë¹„ì¶”ì²œ (ë¹„ìš© ë‚­ë¹„)

---

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘

### í™˜ê²½ ì„¤ì •
```bash
git clone https://github.com/X-iZhang/Libra.git
cd Libra
conda create -n libra python=3.10 -y
conda activate libra
pip install -e ".[train,eval]"
pip install flash-attn --no-build-isolation
```

### ë°ì´í„° ë‹¤ìš´ë¡œë“œ
```bash
# MIMIC-CXR (PhysioNet ê³„ì • í•„ìš”)
# https://physionet.org/content/mimic-cxr-jpg/2.0.0/

# Libra annotations (Google Drive)
mkdir -p ./data
cd ./data
wget <Google Drive links from README>
```

### í•™ìŠµ
```bash
# Stage 1: TAC í•™ìŠµ (16ì¼ @ A6000)
bash scripts/pretrain.sh

# Stage 2: LoRA í•™ìŠµ (9ì¼ @ A6000)
bash scripts/finetune_lora.sh
```

---

## ğŸ“Š ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ

### Libra-v1.0-7b (MIMIC-CXR Findings)

| ì§€í‘œ | ì ìˆ˜ | ì˜ë¯¸ |
|------|------|------|
| **BLEU-1** | 51.3 | ë‹¨ì–´ 51.3% ì¼ì¹˜ |
| **BLEU-4** | 24.5 | 4-gram 24.5% ì¼ì¹˜ |
| **METEOR** | 48.9 | ë™ì˜ì–´ ê³ ë ¤ 48.9% |
| **ROUGE-L** | 36.7 | ë¬¸ì¥ êµ¬ì¡° 36.7% |
| **RaTEScore** | 61.5 | ì„ìƒ ì •í™•ë„ 61.5% â­ |
| **RG_ER** | 37.6 | Entity ì¬í˜„ìœ¨ 37.6% |

**ë¹„êµ**: Med-CXRGen-F ëŒ€ë¹„ BLEU4 +138%, RG_ER +58%

---

## ğŸ† í•µì‹¬ íŠ¹ì§•

### 1. TAC (Temporal Alignment Connector)
- **LFE**: 12 layers â†’ 1 optimal layer
- **TFM**: Temporal reasoning via attention
- **Cosine Similarity Weighting**: 8ìŠ¹ ì ìš©

### 2. 2-Stage í•™ìŠµ
- **Stage 1**: TACê°€ vision-language alignment í•™ìŠµ
- **Stage 2**: LLMì´ report generation í•™ìŠµ
- **ë…ë¦½ì ì´ì§€ë§Œ ìˆœì°¨ì **

### 3. LoRA Aggressive ì„¤ì •
- **Rank**: 128 (high)
- **Alpha**: 256 (scaling=2.0)
- **Target**: 7 modules per layer
- **íŒŒë¼ë¯¸í„°**: 224M (ì „ì²´ì˜ 3.2%)

---

## ğŸ’¡ ì£¼ìš” ë°œê²¬

### Temporal Pairing
- **Same-day íŠ¹ìˆ˜ ì²˜ë¦¬**: ì„ìƒì ìœ¼ë¡œ íƒ€ë‹¹
- **67% í™˜ì**ê°€ 2íšŒ ì´ìƒ ì´¬ì˜
- **100% ì¬í˜„ ê°€ëŠ¥**í•œ ì•Œê³ ë¦¬ì¦˜

### í‰ê°€ ì§€í‘œ
- **RaTEScore ìµœìš°ì„ **: ì„ìƒì˜ í‰ê°€ì™€ highest correlation
- **ì „í†µì  NLP ì§€í‘œ í•œê³„**: ë™ì˜ì–´, ë¶€ì • í‘œí˜„ ë¬´ì‹œ
- **RG_ERë¡œ ì™„ì „ì„± í‰ê°€**: ì¤‘ìš”í•œ findings ëˆ„ë½ íƒì§€

### GPU ìµœì í™”
- **H100 1ê°œ ì¶©ë¶„**: 4.2ì¼, $300
- **6-7Ã— speedup**: A6000 ëŒ€ë¹„
- **Multi-GPU**: ê¸‰í•  ë•Œë§Œ 2ê°œ

---

## ğŸ”— ì›ë³¸ ë¦¬ì†ŒìŠ¤

- **ë…¼ë¬¸**: [arXiv:2411.19378](https://arxiv.org/abs/2411.19378) (ACL 2025)
- **ì½”ë“œ**: [GitHub](https://github.com/X-iZhang/Libra)
- **ëª¨ë¸**: [HuggingFace](https://huggingface.co/X-iZhang/libra-v1.0-7b)
- **MIMIC-CXR**: [PhysioNet](https://physionet.org/content/mimic-cxr-jpg/2.0.0/)
- **ReXrank**: [Leaderboard](https://rexrank.ai)

---

## ğŸ“ ë¬¸ì„œ ì‘ì„± ì •ë³´

- **ì‘ì„±ì¼**: 2025ë…„
- **ëŒ€ìƒ**: Libra ì‚¬ìš©ì, ì—°êµ¬ì, ê°œë°œì
- **ì–¸ì–´**: í•œêµ­ì–´
- **ê¸°ë°˜**: Libra v1.0 (ACL 2025)

---

## ğŸ¤ ê¸°ì—¬

ë³¸ ë¬¸ì„œëŠ” Libra í”„ë¡œì íŠ¸ ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ë¹„ê³µì‹ ê°€ì´ë“œì…ë‹ˆë‹¤.

**ë¬¸ì˜ì‚¬í•­**:
- GitHub Issues: [Libra Repository](https://github.com/X-iZhang/Libra/issues)
- ë…¼ë¬¸ ì €ì: Xi Zhang ì™¸

---

## ğŸ“œ ë¼ì´ì„ ìŠ¤

ë³¸ ë¬¸ì„œëŠ” Libra í”„ë¡œì íŠ¸ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- **ì½”ë“œ**: Apache License 2.0
- **ëª¨ë¸**: HuggingFace Model License

---

**Happy Training! ğŸš€**
